[
{
	"uri": "/1-monitoring-amazon-cloudwatch/1.1-amazon-cloudwatch-hands-on-lab/",
	"title": "Amazon CloudWatch Hands on Lab",
	"tags": [],
	"description": "",
	"content": "Amazon CloudWatch Overview In this lab, you will utilize CloudWatch to track EC2 CPU utilization and set up Alarm based on a configured threshold. The Alarm will trigger a Simple Notification Service(SNS) notification.\nThis Hands on Lab includes the following parts:\nCreate Simple Notification Service (SNS) Topic Launch an Elastic Compute Cloud (EC2) Instance Configure a CloudWatch Alarm Clean up resources "
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/",
	"title": "Amazon RDS MySQL Hands on Lab",
	"tags": [],
	"description": "",
	"content": "Amazon RDS Overview Amazon RDS is a web service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business.\nThis lab requires EC2 Linux Hands-On Lab in advance to complete. This lab will make use of the web server previously created in EC2 lab to connect RDS MySQL.\nThis lab will walk you through the following:\nCreate VPC Security Group Launch an RDS Instance Save RDS Credentials Access RDS from EC2 Create an RDS Snapshot (Optional) Modify RDS Instance Size (Optional) Clean Up Resources "
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/",
	"title": "Amazon S3 Hands-On Lab",
	"tags": [],
	"description": "",
	"content": "Amazon S3 Overview Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. It can be used to store and retrieve any amount of data, at any time, from anywhere on the web.\nThis lab is designed to demonstrate how to interact with S3 through the AWS Console and how-to setup access to objects within a private S3 bucket to be viewed through an EC2 web host.\nThis lab will walk you through the following:\nCreating a bucket in S3 Adding objects to your S3 bucket Working with objects in the S3 console Accessing objects stored in S3 Enabling bucket versioning Setting up a Lifecycle Policy Cleanup: Deleting the objects and the S3 bucket Lab Text Legend\n\u0026ldquo;Quoted Text with a gray highlight\u0026rdquo; - References a heading, title or name in the console. Bold Text - Represents a button, link or dropdown that you will need to click on or select. Inline code - Text you need to copy for input (Longer text will have a copy button, shorter text will just be highlighted). Inline code [with brackets] - The text in brackets represents text that requires a unique input. The required input will be called out in the brackets. Prerequisites:\nIf you want to view the images you upload to your S3 bucket and see versioning in action later in the lab, you will need a web host. This web host feature will allow you to interact with your S3 bucket in a real-world environment. Click on the template link button below to build the web host in EC2 using CloudFormation\nDownload and launch the CloudFormation template\n1.Download the \u0026ldquo;S3-General-ID-Lab.yaml\u0026rdquo; CloudFormation template by right-clicking on this link and save it to your local hard drive.\n2.In the AWS Console search for CloudFormation or select the Services menu and click on CloudFormation under \u0026ldquo;Management \u0026amp; Governance\u0026rdquo;.\n3.In the CloudFormation console select the Create stack button and then select With new resources (standard).\n4.Under Template source select Upload a template file and then select the Choose file button. Select the \u0026ldquo;S3-General-ID-Lab.yaml\u0026rdquo; template file you downloaded in the first step. Once you have selected the template file click on the Next button.\n5.On the Specify stack details page, fill in the following fields:\na. Under \u0026ldquo;Stack name\u0026rdquo; name your stack [Your Initials]-S3-Web-Host.\nb. You can leave \u0026ldquo;AmiID\u0026rdquo; as the default, resulting in the use the most recent version of this AMI.\nc. Under \u0026ldquo;InstanceType\u0026rdquo; select the t2.micro (free tier) or you can choose the m5.large if you have an issue with the t2.micro for any reason.\nd. Under \u0026ldquo;MyIP\u0026rdquo; input the IP address of your local machine followed by a /32. This will lock down HTTP port 80 to your machine. You can find your local IP by searching What is my IP.\ne. Under \u0026ldquo;MyVPC\u0026rdquo; select the VPC you want to use to setup the instance. In most accounts the default VPC will be a good choice and in new AWS accounts it will be the only choice.\nf. Under \u0026ldquo;PublicSubnet\u0026rdquo; select a subnet within your VPC that has internet access. A public subnet is defined by a subnet having a route to the internet gateway within its route table. By default, the default VPC subnets are all public.\n6.Once you are done entering the details above, click on Next. On the next page, \u0026ldquo;Configure stack options\u0026rdquo;, you can leave \u0026ldquo;Tags\u0026rdquo;, \u0026ldquo;Permissions\u0026rdquo;, and \u0026ldquo;Advanced options\u0026rdquo; as default and select Next.\n7.Wait till the \u0026ldquo;Logical ID\u0026rdquo; \u0026ldquo;[Your Initials]-S3-Web-Host\u0026rdquo; shows a status of \u0026ldquo;CREATE_COMPLETE\u0026rdquo;. You will need to click on the refresh button to get the updated status of your stack.\nThe CloudFormation stack creation should be completed in about 3 minutes.\nThe CloudFormation script has now created the architecture below:\nConfirm the successful setup of your instance\nYou should now see the \u0026ldquo;S3 Hands-On Lab\u0026rdquo; page which we will use later in the lab.\nIf the page does not load it is recommended you wait for your instance \u0026ldquo;Status check\u0026rdquo; to show \u0026ldquo;2/2 checks passed\u0026rdquo; and then try again.\n"
},
{
	"uri": "/",
	"title": "AWS General Immersion Day",
	"tags": [],
	"description": "",
	"content": "AWS General Immersion Day Basic Modules In this basic modules, you can learn various functions of each AWS foundational service.\nBasic modules consist of the following agenda:\nMonitoring - Amazon CloudWatch Database - Amazon RDS Storage - Amazon S3 We will be doing this lab in US West (Oregon) region or any region you like.\n"
},
{
	"uri": "/1-monitoring-amazon-cloudwatch/1.1-amazon-cloudwatch-hands-on-lab/1.1.1-create-sns-topic/",
	"title": "Create SNS Topic",
	"tags": [],
	"description": "",
	"content": "Create Simple Notification Service (SNS) Topic First, we will set up a topic for notifying our email address\n1.In AWS Console, search Simple Notification Service or click Amazon SNS\n2.On the left side of the page, choose Topics or click Next step button as shown in the screen below.\n3.When Create topic opens. Choose Standard and includes your name and optionally a Display Name.\nScroll to the bottom of the screen and click Create topic. 4.In the topic’s specific dashboard. Click Create subscription on the right side of the screen.\n5.In the Protocol, choose Email and enter a working email address you are able to access. Click Create Subscription.\nWhen Subscription to Ko-topic created successfully.\n6.Check your email, open the email name AWS Notification – Subscription Confirmation and click Confirm subscription link.\nThis pape show like this\n7.Check Your subscription if the value on the status has changed from PendingConfirmatio to Confirmed.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/create-vpc-security-group/",
	"title": "Create VPC Security Group",
	"tags": [],
	"description": "",
	"content": "Create VPC Security Group Prerequisite: EC2 Linux Hands-On Lab\nIn EC2 Linux Hands-On Lab, we launched a web server EC2 instance with the security group, Immersion Day - Web Server, that allows TCP 80 for the web server.\nFirst, we will create a new VPC security group, Immersion Day - DB Tier, for our database tier that only allows traffic from our web tier.\n1.In the VPC dashboard, click Security Groups, then the Create Security Group button. Type Security group name and Description as below and keep the VPC setting to the same VPC you’ve launched your EC2 instance in.\n# Key Value 1 Security group name Immersion Day DB Tier 2 Description Immersion Day DB Tier 3 VPC VPC-xxxxxx (default) 3.Under Inbound Rules, click Add rule button. Add a new inbound rule for the EC2 server(s) in our web tier. The type should be MySQL/Aurora (3306), the protocol TCP (6), and in the source box, type the name of the security group to which your EC2 instance belongs. While you’re typing, a list of security group(s) that match that name should be presented to you. Select your security group.\n4.Set Name tag and group name to Immersion Day DB Tier. Then, scroll down and click Create security group button. This will create the Security group for your RDS instance.\n5.When successful.\n"
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/creating-a-bucket-in-s3/",
	"title": "Creating a bucket in S3",
	"tags": [],
	"description": "",
	"content": "Every object in Amazon S3 is stored in a bucket. Before you can store data in Amazon S3 you must create a bucket. You are not charged for creating a bucket; you are only charged for storing objects in the bucket and for transferring objects in and out of the bucket.\n1.To find and open S3 you can use the search bar or click on the Services link in the upper left-hand corner of the screen to bring up the services menu. Under the \u0026ldquo;Storage\u0026rdquo; heading select S3 or open the Amazon S3 console here\n2.Click the Create Bucket button. You will be taken to the \u0026ldquo;Create bucket\u0026rdquo; page to begin setting up your bucket.\n3.Enter a name in the \u0026ldquo;Bucket name\u0026rdquo; field. The bucket name you choose must be unique across all existing bucket names in Amazon S3. One way can make your bucket name unique is by prefixing your bucket name with your initials and your organization\u0026rsquo;s name. e.g. [your initials]-[your org]-s3-lab Bucket names must comply with the following requirements:\nCan contain lowercase letters, numbers, periods (.) and dashes (-) only (No UPPERCASE letters!) Must start with a number or letter Must be between 3 and 255 characters long Must not be formatted as an IP address (e.g., 265.255.5.4) In the Region drop-down list select the same region where you setup your web host with CloudFormation. The next section is \u0026ldquo;Block Public Access settings for this bucket\u0026rdquo;. We will be working with a private bucket so leave Block all public access checked. You could set your bucket up for public access giving users and applications the ability to access the objects within the bucket via a unique DNS address. We will not be doing that for this lab. In one later section, we will go through the process of setting up access to your objects without making the bucket public.\n5.You can leave the rest of the settings as default for now, we will be enabling versioning on our bucket later in the lab. Now click on the Create bucket button.\n6.You will now be back on the page showing all your S3 buckets, click on the name of the bucket you just created.\n7.You will now be back on the page showing all your S3 buckets, click on the name of the bucket you just created. Your bucket should show \u0026ldquo;Objects (0)\u0026rdquo;.\n"
},
{
	"uri": "/1-monitoring-amazon-cloudwatch/",
	"title": "Monitoring - Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Amazon CloudWatch Amazon CloudWatch\nIt is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch provides you with data and actionable insights to monitor your applications. Collects monitoring and operational data in the form of logs, metrics, and events. Detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly. Use Cases Infrastructure monitoring and troubleshootingHeader anchor link. Application monitoring. Log analytics Improve operational performance and resource optimization "
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/adding-object-to-your-s3-bucket/",
	"title": "Adding objects to your S3 bucket",
	"tags": [],
	"description": "",
	"content": "Now that you\u0026rsquo;ve created a bucket, you\u0026rsquo;re ready to add objects to it An object can be any kind of file: a text file, a photo, a video, etc. When you add a file to Amazon S3, you have the option of including metadata with the file and setting permissions to control access to the file.\n1.You are going to upload 7 photos into your bucket. Download this zip file and extract the photos onto your local hard drive: photos.zip\n2.In your zip file confirm you have seven files named \u0026ldquo;photo1.jpg\u0026rdquo; through \u0026ldquo;photo7.jpg\u0026rdquo; and a directory named \u0026ldquo;V2\u0026rdquo; containing a file named \u0026ldquo;photo1.jpg\u0026rdquo;.\n3.In your new bucket\u0026rsquo;s overview page click Upload under the \u0026ldquo;Objects\u0026rdquo; tab.\n4.Then click on the Add Files button to select your files for upload. Upload the files \u0026ldquo;photo1.jpg\u0026rdquo; through \u0026ldquo;photo7.jpg\u0026rdquo; from the root of the \u0026ldquo;photos\u0026rdquo; folder, ignoring \u0026ldquo;photo1.jpg\u0026rdquo; in the \u0026ldquo;V2\u0026rdquo; folder as we will use it in a later section of this lab.\n5.After you have selected the image files, the \u0026ldquo;Upload\u0026rdquo; dialogue then should show the files you\u0026rsquo;ve selected to upload.\nHere is the architecture you have built so far:\n"
},
{
	"uri": "/2-database-amazon-rds/",
	"title": "Database - Amazon RDS",
	"tags": [],
	"description": "",
	"content": "Database - Amazon RDS AWS provides the broadest selection of purpose-built ]databases for all your application. 15+ purpose-built database engines allow you to save, grow, and innovate faster.\nHundreds of thousands of customers rely on AWS databases which have the following characteristics:\nPurpose-built database Performance at scale Fully managed Secure \u0026amp; highly available Customers across industries turn to AWS purpose-built databases to power their most important applications such as:\nInternet Scale Applications Real-time applications Open source applications Enterprise applications "
},
{
	"uri": "/1-monitoring-amazon-cloudwatch/1.1-amazon-cloudwatch-hands-on-lab/1.1.2-launch-an-ec2-instance/",
	"title": "Launch an EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Launch an Elastic Compute Cloud (EC2) Instance In this step, you will launch an EC2 instance and configure the User Data to install and launch the stress tool.\n1.Log in to Amazon EC2 and click EC2 Dashboard, choose Launch instance like this picture.\n2.In the Quick Start section, select the Amazon Linux AMI.\n3.Scroll down, click Advanced details. In User data copy this\n#!/bin/sh yum -y update\ramazon-linux-extras install epel -y\ryum install stress -y\rstress -c 1 --backoff 300000000 -t 30m The test script will run a workload generator tool stress designed to subject a system to a configurable measure of CPU. We configure stress to spawn 1 worker with a timeput of 300000000 microseconds or 5 mins for 30 minutes.\n4.In Instance type choose t2.micro. In Key pair(login) choose Proceed without a key pair (Not recommended)\n5.In Name and tags click Add additional tags. Write Name in key and in Value you can name yours in this format: “[Your Name] Server”. In this lab, we use KO Server\n6.Click Launch instance in the box on the right side of the screen\n7.Click View Instances button in the lower right-hand portion of the screen to view the list of EC2 instances. Once your instance has launched, you will see your server as well as the Availability Zone the instance is in.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/launch-an-rds-instance/",
	"title": "Launch an RDS Instance",
	"tags": [],
	"description": "",
	"content": "Launch an RDS Instance Now that our VPC security group for Database is ready, let’s configure and launch a MySQL RDS Instance.\n1.Sign into the AWS Management Console and open the Amazon RDS console\n2.Click on Create database\n3.For Choose a database creation method, select Standard option. With Standard Create, you setup the configurations for your database. Select MySQL in Engine Options.\nEasy Create option provides recommended best-practices configurations to get started with deploying databases.\n4.When you select MySQL as your database engine, the latest version will be automatically selected for you. For this lab, select MySQL version 5.7.X.\n5.For Template, there are three options available: Production, Dev/Test and Free Tier. For the lab purpose, we will select Free Tier.\n6.In Settings section, fill in the following for each field\n# Parameter Value 1 DB Instance Identifier awsdb 2 Master Username awsuser 3 Master Password awspassword 7.In the Storage section, select the Storage Type as General Purpose SSD. You can select or deselect the option for Auto Scaling for the lab purposes.\n8.In the Connectivity section:\n# Parameter Value VPC Default VPC Additional connectivity configuration Subnet Group default Publicly accessible No VPC Security Group(s) Select Choose existing VPC security groups, then pick Immersion Day DB Tier Availability Zone No preference Database port 3306 9.For Database authentication,\n10.Expand on Additional Configuration.\n11.Review your settings and click Create database.\n12.n the RDS Dashboard, monitor your new DB instance until the status changes from creating to backing up to available.\nThis may take up to 5 minutes as the database is being created and backed up.\n"
},
{
	"uri": "/1-monitoring-amazon-cloudwatch/1.1-amazon-cloudwatch-hands-on-lab/1.1.3-configure-a-cloudwatch-alarm/",
	"title": "Configure a CloudWatch Alarm",
	"tags": [],
	"description": "",
	"content": "Configure a CloudWatch Alarm 1.In the EC2 Console, click the checkbox next to your server name to view details about this EC2 instance. Click Actions \u0026raquo; Monitor and troubleshoot \u0026raquo; Manage detailed monitoring.\nAnd then check Enable button under Detailed monitoring to provide monitoring data at a 1 minute interval vs. the default of 5 minutes. Click Save\n2.Click on Actions \u0026raquo; Monitor and troubleshoot \u0026raquo; Manage CloudWatch alarms.\n3.Select Create an alarm. Under Alarm notification, select the SNS topic created in Part 1. In the Alarm thresholds section, set the values as shown below and then click Create.\n4.In the top left area of the AWS Console, select Services click CloudWatch. Click Alarms in the left pane of the Console and select All alarms. Check the State of your alarm. It most likely says INSUFFICIENT_DATA(because you just created it) and later it will say OK. You can also use instance-id to filter the alarms for your EC2 instance.\n5.In the CloudWatch Console, select Metrics in the left pane. Select the All Metrics tab and paste your Instance ID into the filter.\n6.Click on Per-Instance Metrics and then add an additional filter CPU. Select CPUUtilization metric. Click on Graphed metrics button and change the Period to 1 Minute. Change the graph interval to a custom value of 30m and select Auto refresh of 1 min.\n7.After 5 minutes, the stress tool will begin to simulate CPU workload and trigger the Alarm once the threshold is reached. You can view the Alarm state in the CloudWatch console under Alarms. Select All alarms and you should be able to see the configured alarm. If you setup an email notification, you will receive an email alert when the Alarm is triggered.\nCongratulations! You have successfully configured a CloudWatch Alarm!\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/save-rds-credentials/",
	"title": "Save RDS Credentials",
	"tags": [],
	"description": "",
	"content": "Save RDS Credentials to AWS Secrets Manager The web server you created contains sample code for a simple address book. We must tell the sample code how to find the database and connect to it. We will store this information in AWS Secrets Manager.\nIn this section, we will create a secret containing the database connection information. Later, we will grant permission to the web server to retrieve this secret.\n1.In the console, open the AWS Secrets Manager . Click Store a new secret.\n2.Under Secret Type, choose Credentials for Amazon RDS database. Provide the user name and password you entered when you created the database.\n3.Under Database, choose the database you just created. Click Next.\n4.Name your secret, mysecret. The sample code is written to ask for the secret by this specific name. Click Next.\n5.Review your choices. Click Store.\n"
},
{
	"uri": "/3-storage-amazon-s3/",
	"title": "Storage - Amazon S3",
	"tags": [],
	"description": "",
	"content": "Storage - Amazon S3 AWS offers a complete range of services for you to store, access, govern, and analyze your data to reduce costs, increase agility, and accelerate innovation.\nSelect from object storage, file storage, and block storage services, backup, and data migration options to build the foundation of your cloud IT environment.\nLet\u0026rsquo;s get started: S3 Hands-On Lab "
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/working-with-objects-in-the-s3-console/",
	"title": "Working with objects in the S3 Console",
	"tags": [],
	"description": "",
	"content": "Within the S3 console, you have the ability to work with the objects in a graphic UI. The S3 console gives you many familiar file system commands. You can rename, move, copy, delete, and view; to name a few.\nYou also the have the ability to create a folder (also known as a \u0026ldquo;prefix\u0026rdquo;), add or remove metadata, edit the storage class and copy the S3 URI or URL of the object. We will run through a few of these actions in this section of the lab.\nMoving an object in S3\nWith the move action in the S3 console, you can move an object to a folder (prefix) in the same bucket, to another bucket/prefix, or to an access point. For this example, we will create a new folder (prefix) and move the photo7.jpg object.\n1.In your bucket\u0026rsquo;s overview page select the Create folder button.\n2.In the \u0026ldquo;Folder name\u0026rdquo; field put in the name \u0026ldquo;photo7\u0026rdquo; and then click on the Create folder button.\n3.Now select the \u0026ldquo;photo7.jpg\u0026rdquo; object, click on the Actions dropdown, and select Move. Under \u0026ldquo;Destination type\u0026rdquo; select Bucket and then click on Browse S3. In the \u0026ldquo;Destination\u0026rdquo; popup select the photo7/ folder (prefix) and then click on Choose destination. Now that the destination has been set you can click on the Move button to move the object.\n4.You should then be taken to the \u0026ldquo;Move: status\u0026rdquo; page showing that you \u0026ldquo;Successfully moved objects\u0026rdquo;. Click on the Close button to leave the page.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/access-rds-from-ec2/",
	"title": "Access RDS from EC2",
	"tags": [],
	"description": "",
	"content": "Allow the web server to access the secret Now that you have created a secret, you must give your web server permission to use it. To do this, we will create a Policy that allows the web server to read a secret. We will add this policy to the Role you previously assigned to the web server.\n1.If you have not already done so, create an IAM Instance Profile as described in Connect to your Linux instance using Session Manager.\n2.Sign in to the AWS Management Console and open the IAM console . In the navigation pane, choose Policies, and then choose Create Policy.\n3.Click Choose a service. Type Secrets Manager into the search box. Click Secrets Manager.Under Access level, click on the carat next to Read and then check the box by GetSecretValue. Click on the carat next to Resources. For this lab, select All resources. Click Next: Tags.\n4.On the Review Policy* screen, give your new policy the name ReadSecrets. Click Create policy.\n5.In the navigation pane, choose Roles and type SSMInstanceProfile into the search box. This is the role you created previously in Connect to your Linux instance using Session Manager. Click SSMInstanceProfile.\n6.Under Permissions policies, click Attach policies.\n7.Search for the policy you created called ReadSecrets. Check the box and click Attach policy.\nTry the Address Book 1.Navigate to the EC2 console and find the web server you launched in the EC2 Linux Hands-On Lab. Note your web server\u0026rsquo;s public IP.\n2.Open a new tab and reconnect to your web server\u0026rsquo;s public IP. Click RDS.You should now see a simple page displaying all of the information from the database you just created.\nThis is a very basic example of a simple address book interacting with a MySQL database managed by AWS. RDS can support much more complicated relational database scenarios, but we hope this simple example will suffice to demonstrate the point.\nFeel free to play around with the address book and add/edit/remove content from your RDS database by using the Add Contact, Edit, and Remove links in the Address Book.\nGreat Job: You have successfully deployed and utilized an AWS managed MySQL database!!! Would you like to continue your learning? See the two below optional labs you can try out around snapshots and changing instance size.\n"
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/accessing-objects-stored-in-s3/",
	"title": "Accessing objects stored in S3",
	"tags": [],
	"description": "",
	"content": "Creating IAM Role for your EC2 Instance Our EC2 instance we created at the beginning of the lab is a web server that has already been setup to view our objects, we will need to provide it three things:\nPermission to access to our new S3 bucket: Since our S3 bucket is private, we will do this by creating a role with the Identity and Access Management (IAM) service. You can find more information about IAM Roles here. The name of our bucket. The region hosting our bucket. 1.Within the AWS console search for \u0026ldquo;IAM\u0026rdquo; under \u0026ldquo;Services\u0026rdquo; or follow this link.\n2.From the left-hand menu select Roles.\n3.Click on the Create role button.\n4.Under \u0026ldquo;Trusted entity type\u0026rdquo; make sure AWS service is selected and then under \u0026ldquo;Use case\u0026rdquo; select EC2. Then click on the Next button.\n5.Next, we will create a permission policy for our role. A permission policy defines what actions the role can take and what resources the role can use. Click on Create Policy. (This will open another tab in your browser)\n6.On the \u0026ldquo;Create Policy\u0026rdquo; page under the \u0026ldquo;Visual editor\u0026rdquo; tab: a. Under \u0026ldquo;Service\u0026rdquo; select the Choose a service link. Type S3 in the search field and then select S3.\nb. Under \u0026ldquo;Actions\u0026rdquo; type GetObject in the \u0026ldquo;Filter actions\u0026rdquo; search. Put a check in the box next to \u0026ldquo;GetObject\u0026rdquo; only. GetObject will restrict our EC2 instance to only being able to read objects within the bucket you select in the next step.\nc. Next to \u0026ldquo;Resources\u0026rdquo; click on Specify object resource ARN for the GetObject action. Make sure Specify is selected and then click on Add ARN. (ARN stands for \u0026ldquo;Amazon Resource Name\u0026rdquo;)\nd. In the dialogue that opens type in or paste [your-bucket-name] in the \u0026ldquo;Bucket name *\u0026rdquo; field.\ne. For the \u0026ldquo;Object name *\u0026rdquo; field select the check box next to \u0026ldquo;Any\u0026rdquo; and then click on Add.\n7.Now return to your previous browser tab to finish creating your role. Click the refresh button on the right to load your newly created policy. In the \u0026ldquo;Filter policies\u0026rdquo; field search for S3. Select the policy you created in the previous steps, [your initials]-EC2-S3-Access, and then select Next.\n8.We will not be tagging this role so you can click on Next: Review. 9.Name your role [your initials]-EC2-S3-Access-Role and click on Create role.\nAttach your new role to your EC2 instance\nYou have successfully created a role that will give your EC2 web host access to read objects within your S3 bucket. You will need to attach it to your EC2 instance.\n1.Under \u0026ldquo;Services\u0026rdquo; select EC2.\n2.Click on Instances in the left-hand menu.\n3.Find your instance that you named \u0026ldquo;[your initials] - Web Server\u0026rdquo;.\n4.Right-click on your instance, select Security and then Modify IAM Role. This can also be accomplished by selecting your instance and selecting the Actions button.\n5.From the \u0026ldquo;IAM role\u0026rdquo; dropdown select your role you named [your initials]-EC2-S3-Access-Role and click on Save.\n6.Your instance now has read access to your private S3 bucket.\nView your Objects in a Web Browser\nNavigate to the EC2 \u0026ldquo;Instances\u0026rdquo; page, select your instance \u0026ldquo;[Your Initials]-S3-Web-Host\u0026rdquo; and copy the \u0026ldquo;Public IPv4 DNS\u0026rdquo; address into your clipboard by clicking on the image of two overlapping squares to the left of the Public IPv4 DNS Address. Paste this address into a new tab on your web browser.\nClicking on the open address link under \u0026ldquo;Public IPv4 DNS\u0026rdquo; heading may result in you not being able to see your website. The \u0026ldquo;open address\u0026rdquo; link uses https:// instead of http://, which will result with an error because our website server has not been setup with an SSL cert.\n1.You should now see the \u0026ldquo;S3 Hands-On Lab\u0026rdquo; page where you can input your bucket information.\n2.If you need to look up your bucket name and region that can be found on the S3 Service page. The region should be entered into the form in this format: e.g. us-west-2\n3.Enter your \u0026ldquo;Bucket Name\u0026rdquo; and \u0026ldquo;AWS Region\u0026rdquo; and click Submit. After you click on submit you should then see the images you uploaded earlier in a small gallery. (The image with the red X is intentional, we will update this image in the versioning section of the lab)\nIf you are interested in what is happening behind the scenes: The images are read by your web browser from your private S3 bucket. This is done by creating a presigned URL for each object. (in this case the objects are the images) An object owner can share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to those objects. In this case the security credentials we passed to S3 is the role we created for our EC2 instance. If you directly open one of the images in your browser you will see the very long presigned URL.\nHere is the final architecture of what we have built:\n"
},
{
	"uri": "/1-monitoring-amazon-cloudwatch/1.1-amazon-cloudwatch-hands-on-lab/1.1.4-clean-up-resources/",
	"title": "Clean up Resources",
	"tags": [],
	"description": "",
	"content": "Cleaning up Your Resources Be sure to delete the following resources after you are finished:\nCloudWatch Alarm EC2 instance SNS topic 1.To delete the CloudWatch Alarm, in CloudWatch console select the alarm that you created in this lab. From the Actions menu, select Delete. If successful, it will look like this picture\n2.To delete the EC2 Instance, in EC2 console select the instance that you created in this lab. From the Instance state menu, select Terminate instance. If successful, it will look like this picture\n3.To delete the SNS Topic, in SNS console select the topic you created in this lab. Click Delete.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/create-an-rds-snapshot-optional/",
	"title": "Create an RDS Snapshot (Optional)",
	"tags": [],
	"description": "",
	"content": "Create an RDS Snapshot (Optional) Now is a good time to take a snapshot of your RDS database. Taking a snapshot enables you to back up your DB Instance in a known state as frequently as you wish, and then restore to that specific state at any time.\n1.In the RDS section of the of the AWS management console , select your RDS instance, click on Instance actions and select Take snapshot. Give the snapshot a name, Immersion-day-snapshot and click on Take Snapshot.\nUsing single-instance RDS, you will experience downtime for it takes to make a backup. Since our example database is so small that total time to back up is very small too!\n3.DB snapshots can be found under the Snapshots link on the left side of the screen. Notice that you can easily launch new RDS instances from any previous snapshot by selecting Restore Snapshot from the Actions Menu.\n"
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/enabling-bucket-versioning/",
	"title": "Enabling bucket versioning",
	"tags": [],
	"description": "",
	"content": "Enabling bucket versioning Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.\nYou enable and suspend versioning at the bucket level. After you version-enable a bucket, it can never return to an un-versioned state. But you can only suspend versioning on that bucket.\n1.In the S3 Console, click on the Buckets link in the left-hand menu. Click on the name of the bucket you created earlier in the lab and then select the Properties tab. Under the \u0026ldquo;Bucket Versioning\u0026rdquo; heading select the Edit button.\n2.Select the Enable radio button and then click Save Changes.\nBucket Versioning should now show as \u0026ldquo;Enabled\u0026rdquo;\n3.Now return and refresh the \u0026ldquo;S3 Hands-On Lab\u0026rdquo; page, you will now see the new version of \u0026ldquo;photo1.jpg\u0026rdquo; no longer covered by a red X. The older version of the object remains in your bucket if it needs to be downloaded or restored later.\nWhen you enable Versioning, it will by default accept an unlimited number of versions of a object. One way to manage your object versions is by setting up a Lifecycle Policy, which we will do in the next section.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/modify-rds-instance-size-optional/",
	"title": "Modify RDS Instance Size (Optional)",
	"tags": [],
	"description": "",
	"content": "Modify RDS Instance Size (Optional) You can modify your RDS instance size with a few clicks in the AWS console. Follow the steps below:\n1.Select your RDS DB instance, click Instance actions and then Modify.\n2.Edit to a Large instance (t2.small), and if you want, also increase the database size at the same time. Click Next.\n3.In the next screen, don’t forget to click Apply Immediately – otherwise changes will be queued for the next maintenance window. Then, click on Modify DB Instance.\nYou can change instance sizes up or down at any time. However you cannot shrink a database once you increase it. Just like backups, there will be an outage while you perform these operations. In general, major RDS reconfigurations such as scaling database sizes or machine size take between 4 and 12 minutes.\nBut, Don\u0026rsquo;t forget to clean up your resources to prevent additional charges. "
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/setting-up-a-lifecycle-policy/",
	"title": "Setting up a Lifecycle Policy",
	"tags": [],
	"description": "",
	"content": "Setting up a Lifecycle Policy You can use lifecycle policies to define actions you want Amazon S3 to take during an object\u0026rsquo;s lifetime, e.g. transition objects to another storage class, archiving objects, or deleting objects after a specified period.\nA versioning-enabled bucket can have many versions of the same object, one current version and zero or more noncurrent (previous) versions. Using a lifecycle policy, you can define actions specific to current and noncurrent object versions.\nWe are going to setup a lifecycle policy that will move noncurrent (previous) versions of your objects to the S3 Infrequent Access (IA) tier after 30 days and then delete them 30 days later.\n1.In your bucket\u0026rsquo;s overview page, select the Management tab.\n2.Under \u0026ldquo;Lifecycle rules\u0026rdquo; select the Create lifecycle rule button. This should then open the \u0026ldquo;Create lifecycle rule\u0026rdquo; page.You now have a lifecycle policy that will move previous versions of your objects to S3-IA after 30 days and then delete them 30 days later.\n3.Under \u0026ldquo;Transition noncurrent versions of objects between storage classes\u0026rdquo; select Standard-IA for \u0026ldquo;Choose storage class transitions\u0026rdquo;. Enter 30 for \u0026ldquo;Days after objects become noncurrent\u0026rdquo;.\n4.Under \u0026ldquo;Permanently delete noncurrent versions of objects\u0026rdquo; enter 60. This will delete an object 60 days after it becomes previous versions. (30 days after it is moved to S3-IA.)\n5.Select Create rule when you have finished reviewing the summary.\n6.You now have a lifecycle policy that will move previous versions of your objects to S3-IA after 30 days and then delete them 30 days later.\n"
},
{
	"uri": "/2-database-amazon-rds/amazon-rds-my-sql-hands-on-lab/clean-up-resources/",
	"title": "Clean up Resources",
	"tags": [],
	"description": "",
	"content": "Clean up Resources To delete RDS instance that you have created for the lab, select the RDS instance you have created. From the Actions button top right, click Delete\nYou will see a pop up asking if you are sure to delete the DB instance along with creating final snapshot options.\n"
},
{
	"uri": "/3-storage-amazon-s3/amazon-s3-hands-on-lab/clean-up.-deleting-the-objects-and-the-s3-bucket/",
	"title": "Cleanup: Deleting the objects and the S3 bucket",
	"tags": [],
	"description": "",
	"content": "Deleting your objects (including different versions) and your bucket The deletion of objects and buckets can be done programmatically through the API or via the console. If you no longer need the bucket and the objects you uploaded for this lab, you should delete them so you do not incur further charges on those objects.\nYou could go through the bucket and delete every file individually, but that is unnecessary when we can delete them all with one action.\nDelete all objects using the Empty bucket feature\nIf you want to delete all objects within a bucket at once, you can use the Empty option in the S3 console:\n1.In the S3 console select the radio button to the left of your bucket and then select the Empty button.\n2.You will be taken to the \u0026ldquo;Empty bucket\u0026rdquo; page where it gives you several warnings and requires you to type permanently delete in the confirmation field. Once entered you can then click on the Empty button to remove all the objects permanently. 3.You will then be taken to the \u0026ldquo;Empty bucket: status\u0026rdquo; page with a message that you successfully emptied your bucket. Click on Exit to return to the S3 console\nDelete your bucket\n1.In the S3 console click on your bucket named [your-bucket-name] to open the overview page.\n2.Your bucket should now be empty. Click on the toggle labeled \u0026ldquo;Show Versions\u0026rdquo; to confirm there aren\u0026rsquo;t any previous versions of objects still in your bucket.\n3.Return to the \u0026ldquo;Buckets\u0026rdquo; page in the S3 console, select the radio button to the left of your bucket, and then select the Delete button.\n4.On the \u0026ldquo;Delete bucket\u0026rdquo; page you will need to type in [your-bucket-name] in the text input field and then click on Delete bucket.\n5.Once deleted you will be returned to the S3 buckets page with the message \u0026ldquo;Successfully deleted bucket [your-bucket-name]\u0026rdquo;.\nDeleting your CloudFormation Stack\n1.In the console open CloudFormation under services or with search.\n2.Select the stack named [Your Initials]-S3-Web-Host and then the Delete button.\n3.In the popup select Delete stack.\n4.The stack will take a few minutes to delete, select the refresh button to see the updated status. The stack will no longer be visible when it is deleted.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]